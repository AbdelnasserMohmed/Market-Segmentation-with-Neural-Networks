{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/data.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Initial cleaning and handling missing values\n",
        "data['Description'].fillna('No description', inplace=True)\n",
        "data.dropna(subset=['CustomerID'], inplace=True)\n",
        "data = data[(data['Quantity'] > 0) & (data['UnitPrice'] > 0)]\n",
        "data['InvoiceDate'] = pd.to_datetime(data['InvoiceDate'])\n",
        "\n",
        "# Feature Engineering\n",
        "data['TotalPrice'] = data['Quantity'] * data['UnitPrice']\n",
        "data['Year'] = data['InvoiceDate'].dt.year\n",
        "data['Month'] = data['InvoiceDate'].dt.month\n",
        "data['DayOfWeek'] = data['InvoiceDate'].dt.dayofweek\n",
        "\n",
        "purchase_count = data.groupby('CustomerID')['InvoiceNo'].count().reset_index()\n",
        "purchase_count.columns = ['CustomerID', 'PurchaseCount']\n",
        "data = data.merge(purchase_count, on='CustomerID', how='left')\n",
        "\n",
        "average_purchase_value = data.groupby('CustomerID')['TotalPrice'].mean().reset_index()\n",
        "average_purchase_value.columns = ['CustomerID', 'AvgPurchaseValue']\n",
        "data = data.merge(average_purchase_value, on='CustomerID', how='left')\n",
        "\n",
        "# Encoding and Normalizing\n",
        "data = pd.get_dummies(data, columns=['Country'])\n",
        "scaler = StandardScaler()\n",
        "features_to_normalize = ['Quantity', 'UnitPrice', 'TotalPrice', 'PurchaseCount', 'AvgPurchaseValue']\n",
        "data[features_to_normalize] = scaler.fit_transform(data[features_to_normalize])\n",
        "\n",
        "# Define features and target\n",
        "features = data.drop(columns=['InvoiceNo', 'StockCode', 'Description', 'InvoiceDate', 'CustomerID', 'Year', 'Month', 'DayOfWeek'])\n",
        "target = data['PurchaseCount']  # Example target column\n",
        "\n",
        "# Ensure no NaN values in features and target\n",
        "features.fillna(0, inplace=True)\n",
        "target.fillna(0, inplace=True)\n",
        "\n",
        "# Train-test split\n",
        "X = features.values\n",
        "y = target.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure all data types are float\n",
        "X_train = X_train.astype(float)\n",
        "y_train = y_train.astype(float)\n",
        "X_test = X_test.astype(float)\n",
        "y_test = y_test.astype(float)\n",
        "\n",
        "# Neural Network Model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=features.shape[1], activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Loss: {loss}, Mean Absolute Error: {mae}')\n",
        "\n",
        "# Predictions for market segmentation\n",
        "predictions = model.predict(features.values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpLnyiO-hQcU",
        "outputId": "95d942c6-4b84-4686-b7d5-1b06592f47b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "7958/7958 [==============================] - 19s 2ms/step - loss: 0.0198 - mae: 0.0173 - val_loss: 0.0085 - val_mae: 0.0033\n",
            "Epoch 2/50\n",
            "7958/7958 [==============================] - 22s 3ms/step - loss: 0.1203 - mae: 0.0116 - val_loss: 0.0095 - val_mae: 0.0063\n",
            "Epoch 3/50\n",
            "7958/7958 [==============================] - 18s 2ms/step - loss: 0.1440 - mae: 0.0168 - val_loss: 0.0117 - val_mae: 0.0055\n",
            "Epoch 4/50\n",
            "7958/7958 [==============================] - 18s 2ms/step - loss: 0.1264 - mae: 0.0189 - val_loss: 0.0097 - val_mae: 0.0149\n",
            "Epoch 5/50\n",
            "7958/7958 [==============================] - 19s 2ms/step - loss: 1.1208 - mae: 0.0257 - val_loss: 0.0569 - val_mae: 0.0168\n",
            "Epoch 6/50\n",
            "7958/7958 [==============================] - 18s 2ms/step - loss: 0.6022 - mae: 0.0281 - val_loss: 0.0308 - val_mae: 0.0095\n",
            "Epoch 7/50\n",
            "7958/7958 [==============================] - 18s 2ms/step - loss: 0.4544 - mae: 0.0344 - val_loss: 0.0320 - val_mae: 0.0165\n",
            "Epoch 8/50\n",
            "7957/7958 [============================>.] - ETA: 0s - loss: 1.4790 - mae: 0.0330"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfc9AuDSlQHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}